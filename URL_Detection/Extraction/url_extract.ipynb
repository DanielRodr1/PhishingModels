{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T02:17:37.662322Z",
     "start_time": "2025-05-22T02:17:37.059195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from urllib.parse import urlparse\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "import whois\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import entropy\n",
    "from bs4 import BeautifulSoup"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T02:17:37.683927Z",
     "start_time": "2025-05-22T02:17:37.667321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# -------- Funci贸n de extracci贸n --------\n",
    "def contar_digitos(texto):\n",
    "    return sum(c.isdigit() for c in texto)\n",
    "\n",
    "def obtener_tld(subdominio):\n",
    "    return subdominio.split('.')[-1] if '.' in subdominio else ''\n",
    "\n",
    "def string_entropy(s):\n",
    "    prob = [s.count(c) / len(s) for c in set(s)]\n",
    "    return entropy(prob, base=2)\n",
    "\n",
    "def obtener_google_index(url):\n",
    "    try:\n",
    "        # Extraer solo el dominio\n",
    "        parsed = urlparse(url)\n",
    "        dominio = parsed.hostname  # Esto devuelve algo como \"www.ejemplo.com\"\n",
    "\n",
    "        if dominio is None:\n",
    "            return 0\n",
    "\n",
    "        headers = {\"API-Key\": \"01969e7f-04c8-744a-8245-79c2573fe845\"}  # si tienes una\n",
    "        params = {\"q\": f\"domain:{dominio}\", \"size\": 1}\n",
    "        response = requests.get(\"https://urlscan.io/api/v1/search/\", params=params, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            total = data.get(\"total\", 0)\n",
    "            return int(total > 0)\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def obtener_page_rank(dominio, api_key=\"088o008o0gsgcw8k0444k8wswo84888cc0ck8kg4\"):\n",
    "\n",
    "    # Devuelve un valor entre 0 y 10. Devuelve -1 si hay error.\n",
    "\n",
    "    try:\n",
    "        url = \"https://openpagerank.com/api/v1.0/getPageRank\"\n",
    "        headers = {\"API-OPR\": api_key}\n",
    "        params = {\"domains[]\": dominio}\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            rank = data['response'][0].get(\"page_rank_integer\", -1)\n",
    "            return rank if rank is not None else -1\n",
    "        else:\n",
    "            return -1\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def extraer_features(url):\n",
    "    # Asegura que la URL tenga esquema (http o https)\n",
    "    if not url.startswith((\"http://\", \"https://\")):\n",
    "        url = \"http://\" + url\n",
    "\n",
    "    parsed = urlparse(url)\n",
    "    hostname = parsed.hostname if parsed.hostname else ''\n",
    "    path = parsed.path if parsed.path else ''\n",
    "\n",
    "    features = {}\n",
    "    features['longest_words_raw'] = max([len(word) for word in re.split(r'\\W+', url)]) if url else 0\n",
    "    features['nb_eq'] = url.count('=')\n",
    "    features['length_hostname'] = len(hostname)\n",
    "    features['length_url'] = len(url)\n",
    "\n",
    "    # WHOIS para domain_age\n",
    "    try:\n",
    "        dominio_sin_www = hostname[4:] if hostname.startswith(\"www.\") else hostname\n",
    "        info = whois.whois(dominio_sin_www)\n",
    "        creation_date = info.creation_date\n",
    "        if isinstance(creation_date, list):\n",
    "            creation_date = creation_date[0]\n",
    "        if isinstance(creation_date, datetime):\n",
    "            features['domain_age'] = (datetime.now() - creation_date).days\n",
    "        else:\n",
    "            features['domain_age'] = 0\n",
    "    except:\n",
    "        features['domain_age'] = 0\n",
    "\n",
    "    features['nb_slash'] = url.count('/')\n",
    "    path_words = re.split(r'\\W+', path)\n",
    "    features['longest_word_path'] = max([len(word) for word in path_words]) if path_words else 0\n",
    "    hints = ['secure', 'account', 'update', 'login', 'verify', 'bank', 'confirm']\n",
    "    features['phish_hints'] = sum(hint in url.lower() for hint in hints)\n",
    "    features['nb_dots'] = url.count('.')\n",
    "    host_words = hostname.split('.') if hostname else []\n",
    "    features['shortest_word_host'] = min([len(w) for w in host_words]) if host_words else 0\n",
    "\n",
    "    #  Verificar si est谩 indexada en Google\n",
    "    features['google_index'] = obtener_google_index(url)\n",
    "\n",
    "    tld = obtener_tld(hostname)\n",
    "    subdomain = hostname.split('.')[0] if hostname else ''\n",
    "    features['tld_in_subdomain'] = int(tld in subdomain) if tld else 0\n",
    "    digits_url = contar_digitos(url)\n",
    "    features['ratio_digits_url'] = digits_url / len(url) if len(url) > 0 else 0\n",
    "    features['prefix_suffix'] = int('-' in hostname) if hostname else 0\n",
    "    features['ip'] = int(bool(re.fullmatch(r'(\\d{1,3}\\.){3}\\d{1,3}', hostname)))\n",
    "    features['nb_qm'] = url.count('?')\n",
    "    digits_host = contar_digitos(hostname)\n",
    "    features['ratio_digits_host'] = digits_host / len(hostname) if len(hostname) > 0 else 0\n",
    "    features['nb_www'] = url.lower().count('www')\n",
    "    features['page_rank'] = obtener_page_rank(hostname)\n",
    "\n",
    "    # =======================\n",
    "    #  HTML features agregadas\n",
    "    # =======================\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    except:\n",
    "        soup = BeautifulSoup(\"\", \"html.parser\")\n",
    "\n",
    "    title = soup.title.string.strip().lower() if soup.title and soup.title.string else \"\"\n",
    "    features['domain_in_title'] = int(hostname in title)\n",
    "\n",
    "    links = soup.find_all(\"a\", href=True)\n",
    "    features['nb_hyperlinks'] = len(links)\n",
    "\n",
    "    ext_links = [a for a in links if a['href'].startswith((\"http://\", \"https://\")) and hostname not in a['href']]\n",
    "    features['ratio_extHyperlinks'] = len(ext_links) / len(links) if links else 0\n",
    "\n",
    "    time.sleep(5)  # Para evitar bloqueo por peticiones frecuentes\n",
    "\n",
    "    return features"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T02:17:37.908449Z",
     "start_time": "2025-05-22T02:17:37.886440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------- Cargar CSV --------\n",
    "df_urls = pd.read_csv(\"Data/data_final.csv\")  # Reemplaza con tu ruta"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T02:17:46.362949Z",
     "start_time": "2025-05-22T02:17:37.924256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------- Aplicar extracci贸n --------\n",
    "tqdm.pandas()\n",
    "features_list = df_urls[\"URL\"].progress_apply(extraer_features)\n",
    "df_features = pd.DataFrame(features_list.tolist())"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:08<00:00,  8.41s/it]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T02:17:46.404895Z",
     "start_time": "2025-05-22T02:17:46.397896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#  -------- Unir todo --------\n",
    "df_features['URL'] = df_urls['URL']\n",
    "if 'Label' in df_urls.columns:\n",
    "    df_features['Label'] = df_urls['Label']"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T02:17:46.414897Z",
     "start_time": "2025-05-22T02:17:46.408896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#  Reordenar columnas seg煤n orden original de extracci贸n\n",
    "# orden_columnas = [\n",
    "#     'page_rank','domain_age','google_index', 'shortest_word_host', 'nb_eq', 'ratio_digits_host',\n",
    "#     'nb_slash', 'phish_hints', 'prefix_suffix', 'nb_qm', 'longest_words_raw',\n",
    "#     'tld_in_subdomain', 'nb_dots', 'length_url', 'length_hostname', 'ratio_digits_url',\n",
    "#     'nb_www', 'ip', 'longest_word_path', 'URL', 'Label',\n",
    "# ]\n",
    "\n",
    "# orden_columnas = [\n",
    "#     'page_rank','domain_age','google_index','tld_in_subdomain','longest_words_raw','nb_slash','prefix_suffix','length_url','ratio_digits_url','ratio_digits_host','longest_word_path','nb_eq','phish_hints','nb_www','shortest_word_host','ip','length_hostname','nb_qm','nb_dots','URL', 'Label'\n",
    "# ]\n",
    "\n",
    "orden_columnas = [\n",
    "    'google_index', 'page_rank', 'domain_age', 'nb_hyperlinks', 'nb_qm', 'domain_in_title', 'nb_eq', 'length_hostname', 'longest_word_path', 'tld_in_subdomain', 'ratio_digits_host', 'nb_www', 'ip', 'shortest_word_host', 'ratio_digits_url', 'nb_slash', 'length_url', 'longest_words_raw', 'prefix_suffix', 'nb_dots', 'phish_hints', 'ratio_extHyperlinks','URL','Label'\n",
    "]\n",
    "\n",
    "df_features = df_features[orden_columnas]"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T02:17:46.464779Z",
     "start_time": "2025-05-22T02:17:46.455791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------- Guardar --------\n",
    "df_features.to_csv(\"Data/dataset_procesado.csv\", index=False)\n",
    "print(\"Dataset guardado como dataset_procesado.csv\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset guardado como dataset_procesado.csv\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T02:17:46.499587Z",
     "start_time": "2025-05-22T02:17:46.481782Z"
    }
   },
   "cell_type": "code",
   "source": "df_features",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   google_index  page_rank  domain_age  nb_hyperlinks  nb_qm  domain_in_title  \\\n",
       "0             1          0          52              0      0                0   \n",
       "\n",
       "   nb_eq  length_hostname  longest_word_path  tld_in_subdomain  ...  \\\n",
       "0      0              120                  0                 0  ...   \n",
       "\n",
       "   ratio_digits_url  nb_slash  length_url  longest_words_raw  prefix_suffix  \\\n",
       "0          0.054688         3         128                 35              0   \n",
       "\n",
       "   nb_dots  phish_hints  ratio_extHyperlinks  \\\n",
       "0       10            0                    0   \n",
       "\n",
       "                                                 URL  Label  \n",
       "0  http://signin.eday.co.uk.ws.edayisapi.dllsign....      0  \n",
       "\n",
       "[1 rows x 24 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>google_index</th>\n",
       "      <th>page_rank</th>\n",
       "      <th>domain_age</th>\n",
       "      <th>nb_hyperlinks</th>\n",
       "      <th>nb_qm</th>\n",
       "      <th>domain_in_title</th>\n",
       "      <th>nb_eq</th>\n",
       "      <th>length_hostname</th>\n",
       "      <th>longest_word_path</th>\n",
       "      <th>tld_in_subdomain</th>\n",
       "      <th>...</th>\n",
       "      <th>ratio_digits_url</th>\n",
       "      <th>nb_slash</th>\n",
       "      <th>length_url</th>\n",
       "      <th>longest_words_raw</th>\n",
       "      <th>prefix_suffix</th>\n",
       "      <th>nb_dots</th>\n",
       "      <th>phish_hints</th>\n",
       "      <th>ratio_extHyperlinks</th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://signin.eday.co.uk.ws.edayisapi.dllsign....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  24 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
